\documentclass[11pt,a4paper,titlepage]{article}
\usepackage[a4paper]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{lipsum}
\usepackage{hyperref}

\usepackage{amsmath, amssymb, amsfonts, amsthm, fouriernc, mathtools}
% mathtools for: Aboxed (put box on last equation in align envirenment)
\usepackage{microtype} %improves the spacing between words and letters

\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{epstopdf}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% COLOR DEFINITIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[svgnames]{xcolor} % Enabling mixing colors and color's call by 'svgnames'
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\definecolor{MyColor1}{rgb}{0.2,0.4,0.6} %mix personal color
\newcommand{\textb}{\color{Black} \usefont{OT1}{lmss}{m}{n}}
\newcommand{\blue}{\color{MyColor1} \usefont{OT1}{lmss}{m}{n}}
\newcommand{\blueb}{\color{MyColor1} \usefont{OT1}{lmss}{b}{n}}
\newcommand{\red}{\color{LightCoral} \usefont{OT1}{lmss}{m}{n}}
\newcommand{\green}{\color{Turquoise} \usefont{OT1}{lmss}{m}{n}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% FONTS AND COLORS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    SECTIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{titlesec}
\usepackage{sectsty}
%%%%%%%%%%%%%%%%%%%%%%%%
%set section/subsections HEADINGS font and color
\sectionfont{\color{MyColor1}}  % sets colour of sections
\subsectionfont{\color{MyColor1}}  % sets colour of sections

%set section enumerator to arabic number (see footnotes markings alternatives)
\renewcommand\thesection{\arabic{section}.} %define sections numbering
\renewcommand\thesubsection{\thesection\arabic{subsection}} %subsec.num.

%define new section style
\newcommand{\mysection}{
\titleformat{\section} [runin] {\usefont{OT1}{lmss}{b}{n}\color{MyColor1}} 
{\thesection} {3pt} {} } 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%		CAPTIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{caption}
\usepackage{subcaption}
%%%%%%%%%%%%%%%%%%%%%%%%
\captionsetup[figure]{labelfont={color=Turquoise}}


\makeatletter
\let\reftagform@=\tagform@
\def\tagform@#1{\maketag@@@{(\ignorespaces\textcolor{red}{#1}\unskip\@@italiccorr)}}
\renewcommand{\eqref}[1]{\textup{\reftagform@{\ref{#1}}}}
\makeatother
\usepackage{hyperref}
\hypersetup{colorlinks=true}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% PREPARE TITLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\blue Machine Learning \\
\blueb Classification exercise report - Group 27}
\author{Borna Feldsar \\ Martin Matak \\ Soeren Nickel}
\date{\today}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{document}
\maketitle


\section{Datasets}

\subsection{Dataset \textit{KDD Cup 1998}}
Source: \url{https://www.kaggle.com/c/184702-tu-ml-ws-17-kdd-cup-1998}
\subsubsection{Characteristics of the dataset}
\begin{center}
  \begin{tabular}{| c | c | c | c |}
    \hline
    number of samples & number of dimensions & number of classes & preprocessing needed \\ \hline
    ?? & ?? & 2 & ?? \\
    \hline
  \end{tabular}
\end{center}
\subsubsection{Preprocessing}
...

\subsection{Dataset \textit{Breast cancer}}
Source: \url{https://www.kaggle.com/c/184702-tu-ml-ws-17-breast-cancer}
\subsubsection{Characteristics of the dataset}
\begin{center}
  \begin{tabular}{| c | c | c | c |}
    \hline
    number of samples & number of dimensions & number of classes & preprocessing needed \\ \hline
    ?? & ?? & 2 & ??\\
    \hline
  \end{tabular}
\end{center}
\subsubsection{Preprocessing}
...

\subsection{Dataset \textit{Car evaluation}}
Source: \url{https://archive.ics.uci.edu/ml/datasets/car+evaluation}
\subsubsection{Characteristics of the dataset}
\begin{center}
  \begin{tabular}{| c | c | c | c |}
    \hline
    number of samples & number of dimensions & number of classes & preprocessing needed \\ \hline
    1728 & 6 & 4 & yes \\
    \hline
  \end{tabular}
\end{center}
\subsubsection{Preprocessing}
...

\subsection{Dataset \textit{Detect Malicious Executable(AntiVirus)}}
Source: \url{https://archive.ics.uci.edu/ml/datasets/Detect+Malacious+Executable(AntiVirus)}
\subsubsection{Characteristics of the dataset}
\begin{center}
  \begin{tabular}{| c | c | c | c |}
    \hline
    number of samples & number of dimensions & number of classes & preprocessing needed \\ \hline
    373 & 513 & 2 & yes \\
    \hline
  \end{tabular}
\end{center}
\subsubsection{Preprocessing}
...


\section{Classifiers}
We picked four different classifiers: Naive Bayes, SVM, Decision Tree and Random Forest. Below is explained our motivation why we decided to take those four classifiers.

\subsection{Naive Bayes}
Classifier which comes from family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Although it's mostly used for working with text classification, we picked it as one of our four classifiers. We believe that the conditional independence assumption actually holds and that's why we picked this classifier. Moreover, we assume that it will behave well on smaller data sets. Finally, we think that training time will be shorter than for some more complex classifiers - e.g. SVM.

\subsection{Support Vector Machine - SVM}
SVM is non-probabilistic binary linear classifier which can also perform a non-linear classification using the \textit{kernel trick}, implicitly mapping their inputs into high-dimensional feature spaces. As one of the most popular and oldest \footnote{The original SVM algorithm was invented by Vladimir N. Vapnik and Alexey Ya. Chervonenkis in 1963.} classifiers which is in this area, we had to pick it to try it out and learn more about it. We expect it will behave very well on data sets with only two classes. However, we are aware that we will have to come up with a solution for classifying categorical data with this classifier. Moreover, when used to classify in more then two classes, we expect longer runtime. Finally, we expect SVM to be good at dealing with small data sets, since only support vectors and not all the samples are used to constuct the separating hyperplane. 


\subsection{Decision Tree}
Since we have two classifiers which we assume work well with small data sets, we had to pick one which would work good with large data sets in a reasonable time. That's why we picked this classifier. In addition to that, it is able to handle both numerical and categorical data so we will need less preprocessing when using this classifier. However, we must take care about the depth of the tree because it can easily happen that we finish up with over-complex tree that do not generalize well from the training data. That's why we don't expect it to work well with small data sets.

\subsection{Random Forest}
In order to solve the issue of overfitting of \textit{Decision Tree} classifier, we decided to take an ensamble approach. Therefore, we expect to see the difference (at least on smaller data sets) in between \textit{Random Forest} (RF) and \textit{Decision Tree} (DT) in favour of \textit{Random Forest} (RF) which is statistically significant. However, we are aware that RF models are black box and DT models are white box, but that difference for us is not important in this exercise. Finally, since RF is an ensamble approach, we assume that evaluation time will be greater than using DT as classifier.

\subsection{Performance measures}
Based on slides of this course on \href{tuwel.tuwien.ac.at}{TUWEL} and \href{http://rali.iro.umontreal.ca/rali/sites/default/files/publis/SokolovaLapalme-JIPM09.pdf}{this paper}, we decided to measure performance of our classifiers as follows.

In case of \textbf{binary classification} we  provide \textit{confusion matrix} described in Table \ref{table:confusionMatrix}. Using it, we compute then \textit{accuracy} of our classifier as $\frac{tp + tn}{tp + tn + fp + fn}$ and \textit{AUC} as $\frac{1}{2} (\frac{tp}{tp + fn} + \frac{tn}{tn+fp})$.  With those measures, we will measure the overall effectiveness (avg) of the classifier and classifierâ€™s ability to avoid false classification (AUC).

  \begin{table}
  \centering
  \begin{tabular}{| c | c | c |}
    \hline
    Data class & Classified as \textit{pos} & Classified as \textit{neg} \\ \hline
    \textit{pos} & true positive (\textit{tp}) & false negative (\textit{fn}) \\ \hline
    \textit{neg} & false positive (\textit{fp}) & true negative (\textit{tn}) \\
    \hline
  \end{tabular}
  \caption{Confusion matrix}
  \label{table:confusionMatrix}
  \end{table}

Regarding the \textbf{multi-class classification}, since the greatest number of classes we have is 4, we  provide confusion matrix for every of those classes. Moreover, we provide \textit{average accuracy} of a classifier. Average accuracy is defined as average \textit{accuracy} over all of the classes, i.e. $\frac{\sum_{i=1}^{l}\frac{tp_i + tn_i}{tp_i + fp_i + tn_i + fn_i}}{l}$ where \textit{l} is number of different classes. Additionally, we provide \textit{Error rate} $\frac{\sum_{i=1}^{l}\frac{fp_i + fn_i}{tp_i + fp_i + tn_i + fn_i}}{l}$ as well. Hence, we will see the average per-class effectiveness of a classifier and the average per-class classification error.

Since confusion matrix is provided, it should not be a problem for a reader to come up with some other more complex performance measure - e.g. \textit{FScore} or any other performance measure based on confusion matrix if needed. Therefore, we believe that our performance measures and confusion matrix are sufficient.
\section{Dataset \textit{KDD Cup 1998}}
\subsection{Naive Bayes}
\subsection{Support Vector Machine - SVM}
\subsection{Decision Tree}
\subsection{Random Forest}
\subsection{Comparison of classifiers}

\section{Dataset \textit{Breast cancer}}
\subsection{Naive Bayes}
\subsection{Support Vector Machine - SVM}
\subsection{Decision Tree}
\subsection{Random Forest}
\subsection{Comparison of classifiers}

\section{Dataset \textit{Car evaluation}}
\subsection{Naive Bayes}
\subsection{Support Vector Machine - SVM}
\subsection{Decision Tree}
\subsection{Random Forest}
\subsection{Comparison of classifiers}

\section{Dataset \textit{Detect Malicious Executable(AntiVirus)}}
\subsection{Naive Bayes}
\subsection{Support Vector Machine - SVM}
\subsection{Decision Tree}
\subsection{Random Forest}
\subsection{Comparison of classifiers}

\section{Conclusion and future work}
\end{document}