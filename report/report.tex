\documentclass[11pt,a4paper,titlepage]{article}
\usepackage[a4paper]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{lipsum}
\usepackage{hyperref}

\usepackage{amsmath, amssymb, amsfonts, amsthm, fouriernc, mathtools}
% mathtools for: Aboxed (put box on last equation in align envirenment)
\usepackage{microtype} %improves the spacing between words and letters

\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{epstopdf}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% COLOR DEFINITIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[svgnames]{xcolor} % Enabling mixing colors and color's call by 'svgnames'
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\definecolor{MyColor1}{rgb}{0.2,0.4,0.6} %mix personal color
\newcommand{\textb}{\color{Black} \usefont{OT1}{lmss}{m}{n}}
\newcommand{\blue}{\color{MyColor1} \usefont{OT1}{lmss}{m}{n}}
\newcommand{\blueb}{\color{MyColor1} \usefont{OT1}{lmss}{b}{n}}
\newcommand{\red}{\color{LightCoral} \usefont{OT1}{lmss}{m}{n}}
\newcommand{\green}{\color{Turquoise} \usefont{OT1}{lmss}{m}{n}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% FONTS AND COLORS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    SECTIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{titlesec}
\usepackage{sectsty}
%%%%%%%%%%%%%%%%%%%%%%%%
%set section/subsections HEADINGS font and color
\sectionfont{\color{MyColor1}}  % sets colour of sections
\subsectionfont{\color{MyColor1}}  % sets colour of sections

%set section enumerator to arabic number (see footnotes markings alternatives)
\renewcommand\thesection{\arabic{section}.} %define sections numbering
\renewcommand\thesubsection{\thesection\arabic{subsection}} %subsec.num.

%define new section style
\newcommand{\mysection}{
\titleformat{\section} [runin] {\usefont{OT1}{lmss}{b}{n}\color{MyColor1}} 
{\thesection} {3pt} {} } 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%		CAPTIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{caption}
\usepackage{subcaption}
%%%%%%%%%%%%%%%%%%%%%%%%
\captionsetup[figure]{labelfont={color=Turquoise}}


\makeatletter
\let\reftagform@=\tagform@
\def\tagform@#1{\maketag@@@{(\ignorespaces\textcolor{red}{#1}\unskip\@@italiccorr)}}
\renewcommand{\eqref}[1]{\textup{\reftagform@{\ref{#1}}}}
\makeatother
\usepackage{hyperref}
\hypersetup{colorlinks=true}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% PREPARE TITLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\blue Machine Learning \\
\blueb Classification exercise report - Group 27}
\author{Borna Feldsar \\ Martin Matak \\ Soeren Nickel}
\date{\today}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{document}
\maketitle


\section{Data sets}

Characteristics of our data sets can be found in Table \ref{table:datasets}.

\begin{table}
  \centering
  \begin{tabular}{| c | c | c |}
    \hline
    data set & number of samples & number of dimensions & number of classes \\ \hline
    KDD cup & 2500 & 480 & 2 \\ \hline
    Breast cancer & 143 & 9 & 2 \\ \hline
    Car evaluation & 1728 & 6 & 4 \\ \hline
    Malicious programs & 373 & 513 & 2 \\
    \hline
  \end{tabular}
  \caption{Characteristics of data sets}
  \label{table:datasets}
\end{table}


\subsection{Dataset \textit{KDD Cup 1998}}
Source: \url{https://www.kaggle.com/c/184702-tu-ml-ws-17-kdd-cup-1998}

...

\subsection{Dataset \textit{Breast cancer}\footnote{Source: \url{https://www.kaggle.com/c/184702-tu-ml-ws-17-breast-cancer}}}

In training set there were four instances which were missing value for the same attribute. In test set there was only one instance which was missing value, although for a different attribute. There was no instance which was missing values for two or more attributes.

Using a \textit{multinomial logistic regression} method (80/20 split) we filled up missing values.

For classifiers which work only with numerical data, we did that by simply mapping categorical (string) values to numerical or mapping interval of years to one number, but keeping the ordering. E.g. Interval 10-19 is mapped to 1, 20-29 is mapped to 2 etc.

\subsection{Dataset \textit{Car evaluation}}
Source: \url{https://archive.ics.uci.edu/ml/datasets/car+evaluation}

...

\subsection{Dataset \textit{Detect Malicious Executable(AntiVirus)}}
Source: \url{https://archive.ics.uci.edu/ml/datasets/Detect+Malacious+Executable(AntiVirus)}

...


\section{Classifiers}
We picked four different classifiers: Naive Bayes, SVM, Decision Tree and Random Forest. Below is explained our motivation why we decided to take those four classifiers.

\subsection{Naive Bayes}
Classifier which comes from family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Although it's mostly used for working with text classification, we picked it as one of our four classifiers. We believe that the conditional independence assumption actually holds and that's why we picked this classifier. Moreover, we assume that it will behave well on smaller data sets. Finally, we think that training time will be shorter than for some more complex classifiers - e.g. SVM.

\subsection{Support Vector Machine - SVM}
SVM is non-probabilistic binary linear classifier which can also perform a non-linear classification using the \textit{kernel trick}, implicitly mapping their inputs into high-dimensional feature spaces. As one of the most popular and oldest \footnote{The original SVM algorithm was invented by Vladimir N. Vapnik and Alexey Ya. Chervonenkis in 1963.} classifiers which is in this area, we had to pick it to try it out and learn more about it. We expect it will behave very well on data sets with only two classes. However, we are aware that we will have to come up with a solution for classifying categorical data with this classifier. Moreover, when used to classify in more then two classes, we expect longer runtime. Finally, we expect SVM to be good at dealing with small data sets, since only support vectors and not all the samples are used to constuct the separating hyperplane. 


\subsection{Decision Tree}
Since we have two classifiers which we assume work well with small data sets, we had to pick one which would work good with large data sets in a reasonable time. That's why we picked this classifier. In addition to that, it is able to handle both numerical and categorical data so we will need less preprocessing when using this classifier. However, we must take care about the depth of the tree because it can easily happen that we finish up with over-complex tree that do not generalize well from the training data. That's why we don't expect it to work well with small data sets.

\subsection{Random Forest}
In order to solve the issue of overfitting of \textit{Decision Tree} classifier, we decided to take an ensamble approach. Therefore, we expect to see the difference (at least on smaller data sets) in between \textit{Random Forest} (RF) and \textit{Decision Tree} (DT) in favour of \textit{Random Forest} (RF) which is statistically significant. However, we are aware that RF models are black box and DT models are white box, but that difference for us is not important in this exercise. Finally, since RF is an ensamble approach, we assume that evaluation time will be greater than using DT as classifier.

\subsection{Performance measures}
Based on slides of this course on \href{tuwel.tuwien.ac.at}{TUWEL} and \href{http://rali.iro.umontreal.ca/rali/sites/default/files/publis/SokolovaLapalme-JIPM09.pdf}{this paper}, we decided to measure performance of our classifiers as follows.

In case of \textbf{binary classification} we  provide \textit{confusion matrix} described in Table \ref{table:confusionMatrix}. Using it, we compute then \textit{accuracy} of our classifier as $\frac{tp + tn}{tp + tn + fp + fn}$ and \textit{AUC} as $\frac{1}{2} (\frac{tp}{tp + fn} + \frac{tn}{tn+fp})$.  With those measures, we will measure the overall effectiveness (avg) of the classifier and classifierâ€™s ability to avoid false classification (AUC).

  \begin{table}
  \centering
  \begin{tabular}{| c | c | c |}
    \hline
    Data class & Classified as \textit{pos} & Classified as \textit{neg} \\ \hline
    \textit{pos} & true positive (\textit{tp}) & false negative (\textit{fn}) \\ \hline
    \textit{neg} & false positive (\textit{fp}) & true negative (\textit{tn}) \\
    \hline
  \end{tabular}
  \caption{Confusion matrix}
  \label{table:confusionMatrix}
  \end{table}

Regarding the \textbf{multi-class classification}, since the greatest number of classes we have is 4, we  provide confusion matrix for every of those classes. Moreover, we provide \textit{average accuracy} of a classifier. Average accuracy is defined as average \textit{accuracy} over all of the classes, i.e. $\frac{\sum_{i=1}^{l}\frac{tp_i + tn_i}{tp_i + fp_i + tn_i + fn_i}}{l}$ where \textit{l} is number of different classes. Additionally, we provide \textit{Error rate} $\frac{\sum_{i=1}^{l}\frac{fp_i + fn_i}{tp_i + fp_i + tn_i + fn_i}}{l}$ as well. Hence, we will see the average per-class effectiveness of a classifier and the average per-class classification error.

Since confusion matrix is provided, it should not be a problem for a reader to come up with some other more complex performance measure - e.g. \textit{FScore} or any other performance measure based on confusion matrix if needed. Therefore, we believe that our performance measures and confusion matrix are sufficient.

\subsection{Comparison of classifiers}

For every dataset, comparison in between all four classifiers is done. More specifically, \textit{statistical significance testing} is done using \textbf{paired t-Test} (on \textit{accuracy}) with $\mu_0 = 0$. We find that t-Test is convinient in our case since we use cross-validation as a model validation technique with $k=10$. 

In all our experiments, we have the same \textbf{Null hypothesis: both classifiers are equal}. By convention, we reject null hypothesis and say difference is statistically significant if probability of null hypothesis less than 5\%. To obtain \textit{p-value} we used online calculator\footnote{\url{http://www.socscistatistics.com/pvalues/tdistribution.aspx}}.


\section{Dataset \textit{KDD Cup 1998}}

\subsection{Naive Bayes}
Confusion matrix (10-fold) is in the Table \ref{table:confusionMatrixBayesKDD}. Performance measures with different parameters are in the Table \ref{table:BayesKDD}.
\begin{table}
  \centering
  \begin{tabular}{| c | c |}
    \hline
    1012 & 1386 \\ \hline
    32 & 70 \\
    \hline
  \end{tabular}
  \caption{Confusion matrix using the best settings (Naive Bayes - KDD Cup 1998)}
  \label{table:confusionMatrixBayesKDD}
  \end{table}
  
  \begin{table}
  \centering
  \begin{tabular}{| c | c | c | c | c |}
    \hline
     		-	   & Accuracy & 	ACU 	& Training time [min] & Evaluation time [min] \\ \hline
    param1, param2 &  1 	  &     1		& 		1			  & 		1 		\\ \hline
    param1, param2 & 	1	  & 	1 		& 		1			  &			1 		\\ \hline
    param1, param2 & 	2	  & 	1		& 		1			  &			1		\\ \hline
    param1, param2 &  1       & 	1		& 		1			  &			1			\\
    \hline
  \end{tabular}
  \caption{Performance measures with different parameters (Naive Bayes - KDD Cup 1998)}
  \label{table:BayesKDD}
  \end{table}
  
\subsection{Support Vector Machine - SVM}

Confusion matrix (10-fold) is in the Table \ref{table:confusionMatrixSVMKDD}. Performance measures with different parameters are in the Table \ref{table:SVMKDD}. Time measure are done on 70/30 set, but accuracy and ACU are calculated on 10-fold. In confusion matrix is data with $C = 1.0$.
\begin{table}
  \centering
  \begin{tabular}{| c | c |}
    \hline
   		2397 & 1 \\ \hline
    	102 & 0\\
    \hline
  \end{tabular}
  \caption{Confusion matrix using the best settings (SVM - KDD Cup 1998)}
  \label{table:confusionMatrixSVMKDD}
  \end{table}
  
  \begin{table}
  \centering
  \begin{tabular}{| c | c | c | c | c |}
    \hline
     		-	   & Accuracy & 	AUC 	& Training time [sec] & Evaluation time [sec] \\ \hline
    $C = 1.0$ &  0.9588	  	&     0.5		& 		14.5			  & 		12.5 		\\ \hline
    $C = 2.0$ & 	0.956	  & 	0.5 		& 		15.92			  &			13.2 		\\ \hline
    $C = 0.5$ & 		  0.9592 		& 		0			  &			13.16 & 11.43		\\ 
    \hline
  \end{tabular}
  \caption{Performance measures with different parameters (SVM Bayes - KDD Cup 1998)}
  \label{table:SVMKDD}
  \end{table}

\subsection{Decision Tree}

Confusion matrix (10-fold) is in the Table \ref{table:confusionMatrixDTKDD}. Performance measures with different parameters are in the Table \ref{table:DTKDD}. Time measure are done on 70/30 set, but accuracy and ACU are calculated on 10-fold. In confusion matrix is data with $C = 1.0$. Parameters tuned: max depth of the tree ($maxd$) and minimum samples per leaf ($minl$).
\begin{table}
  \centering
  \begin{tabular}{| c | c |}
    \hline
    2379 & 19  \\ \hline
    101 & 1 \\
    \hline
  \end{tabular}
  \caption{Confusion matrix using the best settings (DT - KDD Cup 1998)}
  \label{table:confusionMatrixDTKDD}
  \end{table}
  
  \begin{table}
  \centering
  \begin{tabular}{| c | c | c | c | c |}
    \hline
     		-	   & Accuracy & 	AUC 	& Training time [sec] & Evaluation time [sec] \\ \hline
    $minl=3, maxd=7$ &  0.9592 	  &     0.5		& 		0.1			  & 		0.03 		\\ \hline
    $minl=2, maxd=2$ & 	0.9588	  & 	0.5		& 		0.1			  &			0.03 		\\ \hline
    $minl=100, maxd=12$ &  0.9592 	  &     0		& 		0.1			  & 		0.03 		\\ \hline
    \hline
  \end{tabular}
  \caption{Performance measures with different parameters (DT - KDD Cup 1998)}
  \label{table:DTKDD}
  \end{table}

\subsection{Random Forest}
Confusion matrix (10-fold) is in the Table \ref{table:confusionMatrixRFKDD}. Performance measures with different parameters are in the Table \ref{table:RFKDD}. Time measure are done on 70/30 set, but accuracy and ACU are calculated on 10-fold. In confusion matrix is data with $C = 1.0$. Parameters tuned: max depth of the tree ($maxd$) and minimum samples per leaf ($minl$).
\begin{table}
  \centering
  \begin{tabular}{| c | c |}
    \hline
    2398 & 0  \\ \hline
    102 & 0 \\
    \hline
  \end{tabular}
  \caption{Confusion matrix using the best settings (RF - KDD Cup 1998)}
  \label{table:confusionMatrixDTKDD}
  \end{table}
  
  \begin{table}
  \centering
  \begin{tabular}{| c | c | c | c | c |}
    \hline
     		-	   & Accuracy & 	AUC 	& Training time [sec] & Evaluation time [sec] \\ \hline
    $minl=3, maxd=7$ &  0.9592 	  &     0		& 		0.13	  & 		0.03 		\\ \hline
    $minl=13, maxd=3$ & 	0.9592 	  &     0		& 		0.12		  & 		0.04	\\ \hline
    $minl=1, maxd=35$ & 	0.9592	  & 	1		& 		0.2		  &			0.03		\\ \hline
    $minl=100, maxd=2$ &  0.9592	  & 	1		& 		0.05		  &			0.03				\\
    \hline
  \end{tabular}
  \caption{Performance measures with different parameters (DT - KDD Cup 1998)}
  \label{table:DTKDD}
  \end{table}

\subsection{Comparison of classifiers}
Comparison of classifiers (using the best settings for every classifier) is done using the paired t-Test and $p$-values can be found in a Table \ref{table:comparisonKDD}.


  \begin{table}
  \centering
  \begin{tabular}{| c | c | c | c | c |}
    \hline
     		-	   & NB 	  & 	SVM 	& DT		 & RF \\ \hline
    NB 			   &  - 	  &     1		& 		1	 & 	1 		\\ \hline
    SVM 		   & 	1	  & 	- 		& 		1	 &	1 		\\ \hline
    DT 			   & 	2	  & 	1		& 		-	 &	1		\\ \hline
    RF 			   &  1       & 	1		& 		1	 &	-			\\
    \hline
  \end{tabular}
  \caption{Performance measures with different parameters for KDD cup dataset.}
  \label{table:comparisonKDD}
  \end{table}




\section{Dataset \textit{Breast cancer}}

\subsection{Naive Bayes}
\subsection{Support Vector Machine - SVM}
\subsection{Decision Tree}
\subsection{Random Forest}
\subsection{Comparison of classifiers}

\section{Dataset \textit{Car evaluation}}

\subsection{Naive Bayes}
\subsection{Support Vector Machine - SVM}
\subsection{Decision Tree}
\subsection{Random Forest}
\subsection{Comparison of classifiers}

\section{Dataset \textit{Detect Malicious Executable(AntiVirus)}}

\subsection{Naive Bayes}
\subsection{Support Vector Machine - SVM}
\subsection{Decision Tree}
\subsection{Random Forest}
\subsection{Comparison of classifiers}

\section{Conclusion and future work}
\end{document}
